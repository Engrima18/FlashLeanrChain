{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain\n",
        "!pip install transformers\n",
        "!pip install pdfminer.six\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "U7l0vswU0EvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import LLMChain\n",
        "from langchain.chains import SequentialChain\n",
        "import torch\n",
        "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
        "from tqdm.auto import tqdm\n",
        "from pdfminer.high_level import extract_text\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "ATvxMbfuqRhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline with hugging face model + langchain"
      ],
      "metadata": {
        "id": "oXjQNhT5lO7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'google/flan-t5-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id) # , load_in_8bit=True not working\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=300\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "VfBKO6MY0uTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_chain(text):\n",
        "\n",
        "  # define the correct style the AI should assume for our task\n",
        "  style = \"\"\"American English \\\n",
        "  you are a very helpful assistant and help a professor of statistics and machine learning in creating material to help pupils in learning\n",
        "  \"\"\"\n",
        "\n",
        "  # define the chain for the summary of our text\n",
        "  summary_template = \"\"\"Summarize the following text using this style: {style}\\\n",
        "  Text: {text}\"\"\"\n",
        "\n",
        "  summary_prompt = ChatPromptTemplate.from_template(summary_template)\n",
        "\n",
        "  summary_chain = LLMChain(prompt=summary_prompt,\n",
        "              llm=local_llm,\n",
        "              output_key=\"summary\"\n",
        "              )\n",
        "\n",
        "  summary = summary_chain.run({\"text\": text, \"style\": style})\n",
        "\n",
        "  # define the chain to formulate the question form the previous summary\n",
        "  question_template = \"\"\"From the the text formulate a proper question using this style: {style}\\\n",
        "  Text: {summary}\"\"\"\n",
        "\n",
        "  question_prompt = ChatPromptTemplate.from_template(question_template)\n",
        "\n",
        "  question_chain = LLMChain(prompt=question_prompt,\n",
        "                      llm=local_llm,\n",
        "                      output_key=\"question\"\n",
        "                      )\n",
        "  question = question_chain.run({\"summary\": summary, \"style\": style})\n",
        "\n",
        "  # define the chain for aswering the above question given the summary from the first block\n",
        "  answer_template = \"\"\"Try to asnwer this question using the information provided in the following text\\\n",
        "  Text: {summary}\\\n",
        "  Question: {question}\"\"\"\n",
        "\n",
        "  answer_prompt = ChatPromptTemplate.from_template(answer_template)\n",
        "\n",
        "  answer_chain = LLMChain(prompt=answer_prompt,\n",
        "                      llm=local_llm,\n",
        "                      output_key=\"answer\"\n",
        "                      )\n",
        "  answer = answer_chain.run({\"question\": question, \"summary\": summary, \"style\": style})\n",
        "\n",
        "  return (question, answer)"
      ],
      "metadata": {
        "id": "E-kBMVxdXLRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# divide text into smaller chunks:\n",
        "def divide_text(text, section_size):\n",
        "    sections = []\n",
        "    start = 0\n",
        "    end = section_size\n",
        "    while start < len(text):\n",
        "        section = text[start:end]\n",
        "        sections.append(section)\n",
        "        start = end\n",
        "        end += section_size\n",
        "    return sections\n",
        "\n",
        "# build the flash cards\n",
        "def build_flashcards(pdf_text,llm):\n",
        "\n",
        "    SECTION_SIZE = 4000\n",
        "    divided_sections = divide_text(pdf_text, SECTION_SIZE)\n",
        "    generated_flashcards = []\n",
        "    for i, text in tqdm(enumerate(divided_sections)):\n",
        "\n",
        "        QA_pair = my_chain(text)\n",
        "        generated_flashcards.append(\" -> \".join(QA_pair))\n",
        "\n",
        "    # # Save the cards to a text file\n",
        "    with open(\"flashcards.txt\", \"w\") as f:\n",
        "        f.write(\"\\n\".join(generated_flashcards))"
      ],
      "metadata": {
        "id": "OVtgmM9elp58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the path of the PDF file\n",
        "pdf_file = glob(\"/content/**/*.pdf\", recursive=True)\n",
        "# extrect text from the file at the path\n",
        "pdf_text = extract_text(pdf_file[0])"
      ],
      "metadata": {
        "id": "28DfRkaNt6Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_flashcards(pdf_text, local_llm)"
      ],
      "metadata": {
        "id": "WgywTdQS0xoU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}